import joblib
import pandas as pd
import shap
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from contextlib import asynccontextmanager
import logging
import os
from dotenv import load_dotenv
load_dotenv(override=True)
# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- 1. Cáº¤U HÃŒNH AI & BIáº¾N TOÃ€N Cá»¤C ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL_ID = "gemini-2.5-flash"  # Sá»­ dá»¥ng model má»›i nháº¥t
GEMINI_CLIENT = None
print(f"DEBUG: Key Ä‘ang dÃ¹ng lÃ : {GEMINI_API_KEY[:10]}...")

# Khá»Ÿi táº¡o Gemini client vá»›i error handling
def init_gemini_client():
    """Khá»Ÿi táº¡o Gemini client vá»›i xá»­ lÃ½ lá»—i"""
    global GEMINI_CLIENT
    try:
        from google import genai
        GEMINI_CLIENT = genai.Client(api_key=GEMINI_API_KEY)
        logger.info("âœ… Gemini API client initialized successfully")
        return True
    except ImportError:
        logger.error("âŒ google-generativeai package not installed")
        logger.info("Install with: pip install google-generativeai")
        return False
    except Exception as e:
        logger.error(f"âŒ Failed to initialize Gemini client: {e}")
        return False


MODELS = {}
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
def get_path(filename):
    return os.path.join(BASE_DIR, filename)

# --- 2. QUáº¢N LÃ VÃ’NG Äá»œI (LIFESPAN) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # [STARTUP]: Cháº¡y khi server báº¯t Ä‘áº§u
    try:
        MODELS['clinical_model'] = joblib.load(get_path('diabetes_model.pkl'))
        MODELS['clinical_scaler'] = joblib.load(get_path('scaler_diabetes.pkl'))
        MODELS['clinical_encoders'] = joblib.load(get_path('label_encoders.pkl'))
        MODELS['clinical_background'] = joblib.load(get_path('x_train_sample.pkl'))

        # Load ML models cho NgÆ°á»i dÃ¹ng (Home)
        MODELS['home_model'] = joblib.load(get_path('diabetes_model_home.pkl'))
        MODELS['home_background'] = joblib.load(get_path('x_train_sample_home.pkl'))

        logger.info("âœ… [Lifespan] ÄÃ£ táº£i táº¥t cáº£ ML Models thÃ nh cÃ´ng!")

        # Initialize Gemini (non-blocking)
        gemini_ok = init_gemini_client()
        if not gemini_ok:
            logger.warning("âš ï¸ Gemini API khÃ´ng kháº£ dá»¥ng - sáº½ sá»­ dá»¥ng AI advice máº·c Ä‘á»‹nh")

    except Exception as e:
        logger.error(f"âŒ [Lifespan] Lá»—i táº£i model: {e}")
        raise

    yield
    MODELS.clear()
    logger.info("ðŸ§¹ [Lifespan] ÄÃ£ giáº£i phÃ³ng bá»™ nhá»›.")


# --- 3. KHá»žI Táº O APP & CORS ---
app = FastAPI(title="DiabeTwin AI Backend", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://v0-frontend-ai-one.vercel.app",
                  "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- 4. SCHEMA Dá»® LIá»†U ---
class ClinicalInput(BaseModel):
    gender: str
    age: int
    smoking_history: str
    hypertension: int
    heart_disease: int
    bmi: float
    hba1c: float
    glucose: int


class HomeInput(BaseModel):
    HighBP: int
    HighChol: int
    CholCheck: int
    BMI: float
    Smoker: int
    Stroke: int
    HeartDiseaseorAttack: int
    PhysActivity: int
    Fruits: int
    Veggies: int
    HvyAlcoholConsump: int
    GenHlth: int
    MentHlth: int
    PhysHlth: int
    DiffWalk: int
    Sex: int
    Age: int


# --- 5. HELPER FUNCTIONS ---

def generate_fallback_advice(prob: float, impacts: list, role: str, raw_data: dict) -> str:
    """Táº¡o lá»i khuyÃªn máº·c Ä‘á»‹nh khi Gemini khÃ´ng kháº£ dá»¥ng"""

    top_3 = sorted(impacts, key=lambda x: abs(x['impact']), reverse=True)[:3]
    risk_level = "cao" if prob > 0.7 else "trung bÃ¬nh" if prob > 0.4 else "tháº¥p"

    advice = f"""**PhÃ¢n tÃ­ch nguy cÆ¡ tiá»ƒu Ä‘Æ°á»ng**

ðŸŽ¯ **Káº¿t quáº£:** Nguy cÆ¡ á»Ÿ má»©c **{risk_level}** vá»›i xÃ¡c suáº¥t {prob * 100:.1f}%

ðŸ“Š **3 Yáº¿u tá»‘ áº£nh hÆ°á»Ÿng lá»›n nháº¥t:**
"""

    for i, impact in enumerate(top_3, 1):
        direction = "tÄƒng" if impact['impact'] > 0 else "giáº£m"
        advice += f"\n{i}. **{impact['feature']}**: {direction} {abs(impact['impact']):.1f}% nguy cÆ¡"

    advice += "\n\nðŸ’¡ **Khuyáº¿n nghá»‹:**\n"

    # Khuyáº¿n nghá»‹ dá»±a trÃªn risk level
    if prob > 0.7:
        advice += """
1. **Kháº©n cáº¥p:** Cáº§n gáº·p bÃ¡c sÄ© chuyÃªn khoa tiá»ƒu Ä‘Æ°á»ng trong vÃ²ng 1 tuáº§n
2. **Kiá»ƒm tra:** XÃ©t nghiá»‡m HbA1c vÃ  Ä‘Æ°á»ng huyáº¿t Ä‘Ã³i ngay
3. **Lá»‘i sá»‘ng:** Báº¯t Ä‘áº§u cháº¿ Ä‘á»™ Äƒn kiÃªng Ã­t Ä‘Æ°á»ng, tÄƒng váº­n Ä‘á»™ng ngay láº­p tá»©c
"""
    elif prob > 0.4:
        advice += """
1. **Theo dÃµi:** Äáº·t lá»‹ch khÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³ 3-6 thÃ¡ng/láº§n
2. **PhÃ²ng ngá»«a:** Äiá»u chá»‰nh cháº¿ Ä‘á»™ Äƒn, tÄƒng váº­n Ä‘á»™ng 30 phÃºt/ngÃ y
3. **Kiá»ƒm tra:** Theo dÃµi cÃ¡c chá»‰ sá»‘ sá»©c khá»e táº¡i nhÃ 
"""
    else:
        advice += """
1. **Duy trÃ¬:** Tiáº¿p tá»¥c lá»‘i sá»‘ng lÃ nh máº¡nh hiá»‡n táº¡i
2. **Kiá»ƒm tra:** KhÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³ hÃ ng nÄƒm
3. **PhÃ²ng ngá»«a:** Giá»¯ cÃ¢n náº·ng á»•n Ä‘á»‹nh, váº­n Ä‘á»™ng Ä‘á»u Ä‘áº·n
"""

    advice += "\n\nâš ï¸ *LÆ°u Ã½: Káº¿t quáº£ chá»‰ mang tÃ­nh tham kháº£o. Vui lÃ²ng tham kháº£o Ã½ kiáº¿n bÃ¡c sÄ© chuyÃªn khoa.*"

    return advice


async def get_gemini_advice(prob: float, impacts: list, role: str, raw_data: dict) -> str:
    # 1. Táº¡o bá»‘i cáº£nh dá»±a trÃªn Role
    if role == "BÃ¡c sÄ©":
        context = """Báº¡n lÃ  má»™t chuyÃªn gia ná»™i tiáº¿t há»— trá»£ bÃ¡c sÄ©. 
        HÃ£y phÃ¢n tÃ­ch dá»¯ liá»‡u lÃ¢m sÃ ng dÆ°á»›i gÃ³c Ä‘á»™ chuyÃªn mÃ´n y khoa, sá»­ dá»¥ng thuáº­t ngá»¯ y há»c."""
        requirement = "HÃ£y Ä‘Æ°a ra nháº­n Ä‘á»‹nh chuyÃªn mÃ´n ngáº¯n gá»n nháº¥t cÃ³ thá»ƒ, lÆ°u Ã½ cÃ¡c chá»‰ sá»‘ nguy hiá»ƒm vÃ  gá»£i Ã½ hÆ°á»›ng Ä‘iá»u trá»‹/xÃ©t nghiá»‡m tiáº¿p theo."
    else:
        context = """Báº¡n lÃ  má»™t bÃ¡c sÄ© gia Ä‘Ã¬nh áº£o thÃ¢n thiá»‡n. 
        HÃ£y giáº£i thÃ­ch káº¿t quáº£ sÃ ng lá»c cho ngÆ°á»i dÃ¹ng bÃ¬nh thÆ°á»ng báº±ng ngÃ´n ngá»¯ dá»… hiá»ƒu, gáº§n gÅ©i."""
        requirement = "HÃ£y giáº£i thÃ­ch cÃ¡c chá»‰ sá»‘ má»™t cÃ¡ch Ä‘Æ¡n giáº£n vÃ  Ä‘Æ°a ra 3-4 lá»i khuyÃªn thay Ä‘á»•i lá»‘i sá»‘ng, thá»±c Ä‘Æ¡n Äƒn uá»‘ng cá»¥ thá»ƒ."

    # 2. XÃ¢y dá»±ng Prompt tá»•ng há»£p
    prompt = f"""
    {context}

    **Káº¿t quáº£ dá»± Ä‘oÃ¡n:**
    - Nguy cÆ¡: {prob * 100:.1f}%
    - CÃ¡c yáº¿u tá»‘ áº£nh hÆ°á»Ÿng (SHAP): {impacts}
    - Dá»¯ liá»‡u chi tiáº¿t: {raw_data}

    **YÃªu cáº§u:**
    {requirement}

    Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, Ä‘á»‹nh dáº¡ng Markdown rÃµ rÃ ng.
    """

    try:
        # Gá»i Gemini API (giá»¯ nguyÃªn logic Ä‘Ã£ cháº¡y OK cá»§a báº¡n)
        response = GEMINI_CLIENT.models.generate_content(
            model=GEMINI_MODEL_ID,
            contents=prompt
        )
        return response.candidates[0].content.parts[0].text
    except Exception as e:
        logger.error(f"Error: {e}")
        return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ Ä‘Æ°a ra lá»i khuyÃªn lÃºc nÃ y."

# --- 6. ENDPOINTS ---

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "models_loaded": len(MODELS) > 0,
        "gemini_available": GEMINI_CLIENT is not None
    }


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "DiabeTwin AI Backend",
        "version": "2.0",
        "endpoints": {
            "health": "/health",
            "clinical": "/api/predict/clinical",
            "home": "/api/predict/home",
            "docs": "/docs"
        }
    }


@app.post("/api/predict/clinical")
async def predict_clinical(data: ClinicalInput):
    """Clinical prediction endpoint (Doctor mode)"""
    try:
        logger.info(f"Clinical prediction request: {data.model_dump()}")

        encoders = MODELS['clinical_encoders']
        scaler = MODELS['clinical_scaler']

        input_list = [
            encoders['gender'].transform([data.gender])[0],
            data.age, data.hypertension, data.heart_disease,
            encoders['smoking_history'].transform([data.smoking_history])[0],
            data.bmi, data.hba1c, data.glucose
        ]

        df = pd.DataFrame([input_list], columns=MODELS['clinical_background'].columns)
        scaled_df = pd.DataFrame(scaler.transform(df), columns=df.columns)

        prob = float(MODELS['clinical_model'].predict_proba(scaled_df)[0][1])

        # SHAP Calculation
        f = lambda x: MODELS['clinical_model'].predict_proba(x)[:, 1]
        background = scaler.transform(MODELS['clinical_background'].sample(100))
        explainer = shap.Explainer(f, background)
        shap_values = explainer(scaled_df)

        impacts = []
        features = ["Giá»›i tÃ­nh", "Tuá»•i", "Huyáº¿t Ã¡p", "Bá»‡nh tim", "HÃºt thuá»‘c", "BMI", "HbA1c", "ÄÆ°á»ng huyáº¿t"]
        for i, val in enumerate(shap_values.values[0]):
            impacts.append({"feature": features[i], "impact": round(val * 100, 2)})

        advice = await get_gemini_advice(prob, impacts, "BÃ¡c sÄ©", data.model_dump())

        excluded_features = ["Giá»›i tÃ­nh", "HÃºt thuá»‘c"]
        frontend_impacts = [i for i in impacts if i["feature"] not in excluded_features]

        result = {
            "probability": round(prob * 100, 2),
            "status": "DÆ¯Æ NG TÃNH" if prob > 0.5 else "Ã‚M TÃNH",
            "risk_level": "ðŸ”´" if prob > 0.7 else "ðŸŸ¡" if prob > 0.3 else "ðŸŸ¢",
            "impacts": frontend_impacts,
            "ai_advice": advice
        }

        logger.info(f"Clinical prediction successful: {prob * 100:.1f}%")
        return result

    except Exception as e:
        logger.error(f"Clinical prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/predict/home")
async def predict_home(data: HomeInput):
    """Home prediction endpoint (User mode)"""
    try:
        logger.info(f"Home prediction request received")

        df = pd.DataFrame([data.dict()])
        prob = float(MODELS['home_model'].predict_proba(df)[0][1])

        f = lambda x: MODELS['home_model'].predict_proba(x)[:, 1]
        explainer = shap.Explainer(f, MODELS['home_background'])
        shap_values = explainer(df)

        display_names = [
            "Huyáº¿t Ã¡p cao", "Cholesterol cao", "Kiá»ƒm tra Chol", "Chá»‰ sá»‘ BMI",
            "HÃºt thuá»‘c", "Äá»™t quá»µ", "Bá»‡nh tim", "Váº­n Ä‘á»™ng", "TrÃ¡i cÃ¢y",
            "Rau xanh", "RÆ°á»£u bia", "Sá»©c khá»e tá»•ng quÃ¡t", "Sá»©c khá»e tÃ¢m tháº§n",
            "Sá»©c khá»e thá»ƒ cháº¥t", "Äi láº¡i khÃ³", "Giá»›i tÃ­nh", "NhÃ³m tuá»•i"
        ]

        impacts = []
        for i, val in enumerate(shap_values.values[0]):
            impacts.append({"feature": display_names[i], "impact": round(val * 100, 2)})

        advice = await get_gemini_advice(prob, impacts, "NgÆ°á»i dÃ¹ng", data.model_dump())
        excluded_features_home = ["Giá»›i tÃ­nh", "RÆ°á»£u bia", "Sá»©c khá»e tÃ¢m tháº§n", "HÃºt thuá»‘c", "TrÃ¡i cÃ¢y", "Rau xanh"]
        frontend_impacts_home = [i for i in impacts if i["feature"] not in excluded_features_home]
        result = {
            "probability": round(prob * 100, 2),
            "status": "NGUY CÆ  CAO" if prob > 0.5 else "AN TOÃ€N",
            "impacts": frontend_impacts_home,
            "ai_advice": advice
        }

        logger.info(f"Home prediction successful: {prob * 100:.1f}%")
        return result

    except Exception as e:
        logger.error(f"Home prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
