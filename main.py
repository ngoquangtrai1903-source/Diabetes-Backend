import joblib
import pandas as pd
import shap
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from contextlib import asynccontextmanager
import logging
import os
from dotenv import load_dotenv
load_dotenv(override=True)
# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- 1. Cáº¤U HÃŒNH AI & BIáº¾N TOÃ€N Cá»¤C ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL_ID = "gemini-2.5-flash"  # Sá»­ dá»¥ng model má»›i nháº¥t
GEMINI_CLIENT = None
print(f"DEBUG: Key Ä‘ang dÃ¹ng lÃ : {GEMINI_API_KEY[:10]}...")

# Khá»Ÿi táº¡o Gemini client vá»›i error handling
def init_gemini_client():
    """Khá»Ÿi táº¡o Gemini client vá»›i xá»­ lÃ½ lá»—i"""
    global GEMINI_CLIENT
    try:
        from google import genai
        GEMINI_CLIENT = genai.Client(api_key=GEMINI_API_KEY)
        logger.info("âœ… Gemini API client initialized successfully")
        return True
    except ImportError:
        logger.error("âŒ google-generativeai package not installed")
        logger.info("Install with: pip install google-generativeai")
        return False
    except Exception as e:
        logger.error(f"âŒ Failed to initialize Gemini client: {e}")
        return False

from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd

# Báº®T BUá»˜C: Pháº£i cÃ³ Ä‘á»‹nh nghÄ©a nÃ y á»Ÿ file Backend
class OutlierClipper(BaseEstimator, TransformerMixin):
    """Há»c ngÆ°á»¡ng clip tá»« train, Ã¡p dá»¥ng cho má»i táº­p. An toÃ n vá»›i joblib."""
    def __init__(self, cols, lower_q=0.01, upper_q=0.99):
        self.cols    = cols
        self.lower_q = lower_q
        self.upper_q = upper_q

    def fit(self, X, y=None):
        # Khi load tá»« pkl, hÃ m nÃ y khÃ´ng cháº¡y, nhÆ°ng class váº«n cáº§n cÃ³ cáº¥u trÃºc nÃ y
        return self

    def transform(self, X, y=None):
        X_ = X.copy()
        if not isinstance(X_, pd.DataFrame):
            X_ = pd.DataFrame(X_)
        # LÆ°u Ã½: Khi load tá»« joblib, self.clip_limits_ Ä‘Ã£ cÃ³ sáºµn dá»¯ liá»‡u tá»« lÃºc train
        for col, (lo, hi) in self.clip_limits_.items():
            if col in X_.columns:
                X_[col] = X_[col].clip(lo, hi)
        return X_

    def get_feature_names_out(self, input_features=None):
        return input_features

MODELS = {}
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
def get_path(filename):
    return os.path.join(BASE_DIR, filename)

# --- 2. QUáº¢N LÃ VÃ’NG Äá»œI (LIFESPAN) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # [STARTUP]: Cháº¡y khi server báº¯t Ä‘áº§u
    try:
        MODELS['preprocessor_clinical'] = joblib.load(get_path('preprocessor_clinical.pkl'))
        MODELS['model_clinical'] = joblib.load(get_path('model_clinical.pkl'))
        MODELS['clinical_background_processed'] = joblib.load(get_path('clinical_background_processed.pkl'))

        # Load ML models cho NgÆ°á»i dÃ¹ng (Home)
        MODELS['home_model'] = joblib.load(get_path('diabetes_model_home.pkl'))
        MODELS['home_background'] = joblib.load(get_path('x_train_sample_home.pkl'))

        logger.info("âœ… [Lifespan] ÄÃ£ táº£i táº¥t cáº£ ML Models thÃ nh cÃ´ng!")

        # Initialize Gemini (non-blocking)
        gemini_ok = init_gemini_client()
        if not gemini_ok:
            logger.warning("âš ï¸ Gemini API khÃ´ng kháº£ dá»¥ng - sáº½ sá»­ dá»¥ng AI advice máº·c Ä‘á»‹nh")

    except Exception as e:
        logger.error(f"âŒ [Lifespan] Lá»—i táº£i model: {e}")
        raise

    yield
    MODELS.clear()
    logger.info("ðŸ§¹ [Lifespan] ÄÃ£ giáº£i phÃ³ng bá»™ nhá»›.")


# --- 3. KHá»žI Táº O APP & CORS ---
app = FastAPI(title="DiabeTwin AI Backend", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://v0-frontend-ai-one.vercel.app",
                  "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- 4. SCHEMA Dá»® LIá»†U ---
class ClinicalInput(BaseModel):
    gender: str
    age: int
    smoking_history: str
    hypertension: int
    heart_disease: int
    bmi: float
    hba1c: float
    glucose: int


class HomeInput(BaseModel):
    HighBP: int
    HighChol: int
    CholCheck: int
    BMI: float
    Smoker: int
    Stroke: int
    HeartDiseaseorAttack: int
    PhysActivity: int
    Fruits: int
    Veggies: int
    HvyAlcoholConsump: int
    GenHlth: int
    MentHlth: int
    PhysHlth: int
    DiffWalk: int
    Sex: int
    Age: int


# --- 5. HELPER FUNCTIONS ---

def generate_fallback_advice(prob: float, impacts: list, role: str, raw_data: dict) -> str:
    """Táº¡o lá»i khuyÃªn máº·c Ä‘á»‹nh khi Gemini khÃ´ng kháº£ dá»¥ng"""

    top_3 = sorted(impacts, key=lambda x: abs(x['impact']), reverse=True)[:3]
    risk_level = "cao" if prob > 0.7 else "trung bÃ¬nh" if prob > 0.4 else "tháº¥p"

    advice = f"""**PhÃ¢n tÃ­ch nguy cÆ¡ tiá»ƒu Ä‘Æ°á»ng**

ðŸŽ¯ **Káº¿t quáº£:** Nguy cÆ¡ á»Ÿ má»©c **{risk_level}** vá»›i xÃ¡c suáº¥t {prob * 100:.1f}%

ðŸ“Š **3 Yáº¿u tá»‘ áº£nh hÆ°á»Ÿng lá»›n nháº¥t:**
"""

    for i, impact in enumerate(top_3, 1):
        direction = "tÄƒng" if impact['impact'] > 0 else "giáº£m"
        advice += f"\n{i}. **{impact['feature']}**: {direction} {abs(impact['impact']):.1f}% nguy cÆ¡"

    advice += "\n\nðŸ’¡ **Khuyáº¿n nghá»‹:**\n"

    # Khuyáº¿n nghá»‹ dá»±a trÃªn risk level
    if prob > 0.7:
        advice += """
1. Kháº©n cáº¥p: Cáº§n gáº·p bÃ¡c sÄ© chuyÃªn khoa tiá»ƒu Ä‘Æ°á»ng trong vÃ²ng 1 tuáº§n
2. Kiá»ƒm tra: XÃ©t nghiá»‡m HbA1c vÃ  Ä‘Æ°á»ng huyáº¿t Ä‘Ã³i ngay
3. Lá»‘i sá»‘ng: Báº¯t Ä‘áº§u cháº¿ Ä‘á»™ Äƒn kiÃªng Ã­t Ä‘Æ°á»ng, tÄƒng váº­n Ä‘á»™ng ngay láº­p tá»©c
"""
    elif prob > 0.4:
        advice += """
1. Theo dÃµi: Äáº·t lá»‹ch khÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³ 3-6 thÃ¡ng/láº§n
2. PhÃ²ng ngá»«a: Äiá»u chá»‰nh cháº¿ Ä‘á»™ Äƒn, tÄƒng váº­n Ä‘á»™ng 30 phÃºt/ngÃ y
3. Kiá»ƒm tra: Theo dÃµi cÃ¡c chá»‰ sá»‘ sá»©c khá»e táº¡i nhÃ 
"""
    else:
        advice += """
1. Duy trÃ¬: Tiáº¿p tá»¥c lá»‘i sá»‘ng lÃ nh máº¡nh hiá»‡n táº¡i
2. Kiá»ƒm tra: KhÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³ hÃ ng nÄƒm
3. PhÃ²ng ngá»«a: Giá»¯ cÃ¢n náº·ng á»•n Ä‘á»‹nh, váº­n Ä‘á»™ng Ä‘á»u Ä‘áº·n
"""

    advice += "\n\nâš ï¸ *LÆ°u Ã½: Káº¿t quáº£ chá»‰ mang tÃ­nh tham kháº£o. Vui lÃ²ng tham kháº£o Ã½ kiáº¿n bÃ¡c sÄ© chuyÃªn khoa.*"

    return advice


async def get_gemini_advice(prob: float, impacts: list, role: str, raw_data: dict) -> str:
    # 1. Táº¡o bá»‘i cáº£nh dá»±a trÃªn Role
    if role == "BÃ¡c sÄ©":
        context = """Báº¡n lÃ  má»™t chuyÃªn gia ná»™i tiáº¿t há»— trá»£ bÃ¡c sÄ©. 
            HÃ£y phÃ¢n tÃ­ch dá»¯ liá»‡u lÃ¢m sÃ ng dÆ°á»›i gÃ³c Ä‘á»™ chuyÃªn mÃ´n y khoa."""

        requirement = """
            HÃ£y tráº£ lá»i theo CHÃNH XÃC format sau (má»—i má»¥c trÃªn 1 dÃ²ng):

    **ÄÃ¡nh giÃ¡ nguy cÆ¡**
    - [Nháº­n Ä‘á»‹nh vá» má»©c Ä‘á»™ nguy cÆ¡]
    - [Giáº£i thÃ­ch vá» cÃ¡c chá»‰ sá»‘ quan trá»ng]

    **Khuyáº¿n nghá»‹ lÃ¢m sÃ ng**
    - [XÃ©t nghiá»‡m cáº§n lÃ m thÃªm]
    - [HÆ°á»›ng Ä‘iá»u trá»‹ Ä‘á» xuáº¥t]
    - [Theo dÃµi cáº§n thiáº¿t]

    **LÆ°u Ã½ Ä‘áº·c biá»‡t**
    - [CÃ¡c chá»‰ sá»‘ cáº§n chÃº Ã½ kháº©n cáº¥p náº¿u cÃ³]
            """
    else:
        context = """Báº¡n lÃ  BÃ¡c sÄ© gia Ä‘Ã¬nh há»— trá»£ ngÆ°á»i dÃ¹ng táº¡i nhÃ . 
HÃ£y giáº£i thÃ­ch káº¿t quáº£ dá»± Ä‘oÃ¡n tiá»ƒu Ä‘Æ°á»ng má»™t cÃ¡ch dá»… hiá»ƒu, gáº§n gÅ©i vÃ  Ä‘áº§y Ä‘á»§ Ä‘á»‹nh lÆ°á»£ng."""

        requirement = f"""
        HÃ£y tráº£ lá»i theo CHÃNH XÃC format sau (má»—i má»¥c trÃªn 1 dÃ²ng):

**ÄÃ¡nh giÃ¡ sá»©c khá»e**
- [Nháº­n Ä‘á»‹nh nguy cÆ¡ dá»±a trÃªn xÃ¡c suáº¥t]
- [Giáº£i thÃ­ch Ã½ nghÄ©a cÃ¡c chá»‰ sá»‘ áº£nh hÆ°á»Ÿng chÃ­nh Ä‘áº¿n dá»± Ä‘oÃ¡n]

**Lá»i khuyÃªn hÃ nh Ä‘á»™ng (Cá»¥ thá»ƒ sá»‘ liá»‡u)**
- [Cháº¿ Ä‘á»™ Äƒn: Ä‚n gÃ¬, bá» gÃ¬, Ä‘á»‹nh lÆ°á»£ng gram/bá»¯a tháº¿ nÃ o?]
- [Váº­n Ä‘á»™ng: Táº­p mÃ´n gÃ¬, bao nhiÃªu phÃºt/ngÃ y, bao nhiÃªu ngÃ y/tuáº§n?]
- [Má»¥c tiÃªu: Cáº§n giáº£m bao nhiÃªu kg, Ä‘Æ°a chá»‰ sá»‘ vá» má»©c bao nhiÃªu?]

**LÆ°u Ã½ quan trá»ng**
- [Dáº¥u hiá»‡u cáº§n Ä‘i khÃ¡m ngay hoáº·c lá»i nháº¯c tÃ¡i khÃ¡m]

# QUY Táº®C Cá» Äá»ŠNH
1. KHÃ”NG chÃ o há»i xÃ£ giao. Báº¯t Ä‘áº§u ngay báº±ng pháº§n ÄÃ¡nh giÃ¡.
2. Má»i lá»i khuyÃªn PHáº¢I gáº¯n liá»n vá»›i con sá»‘ cá»¥ thá»ƒ cá»§a bá»‡nh nhÃ¢n (BMI, Glucose...).
3. Ãt nháº¥t 9-10 gá»£i Ã½ chi tiáº¿t chia Ä‘á»u cho cÃ¡c má»¥c.
                """

    prompt = f"""
    {context}

    **Dá»¯ liá»‡u bá»‡nh nhÃ¢n:**
    - Nguy cÆ¡ tiá»ƒu Ä‘Æ°á»ng: {prob * 100:.1f}%
    - CÃ¡c yáº¿u tá»‘ áº£nh hÆ°á»Ÿng chÃ­nh: {impacts}
    - ThÃ´ng tin chi tiáº¿t: {raw_data}

    **YÃŠU Cáº¦U QUAN TRá»ŒNG:**
    {requirement}

    CHÃš Ã: 
    - Má»—i gá»£i Ã½ pháº£i lÃ  1 cÃ¢u hoÃ n chá»‰nh, cá»¥ thá»ƒ, cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c
    - Sá»­ dá»¥ng dáº¥u - á»Ÿ Ä‘áº§u má»—i dÃ²ng
    - KHÃ”NG thÃªm sá»‘ thá»© tá»±, KHÃ”NG thÃªm header phá»©c táº¡p
    - Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
    """

    try:

        response = GEMINI_CLIENT.models.generate_content(
            model=GEMINI_MODEL_ID,
            contents=prompt
        )
        print("=" * 50, flush=True)
        print("GEMINI RESPONSE:", flush=True)
        print("=" * 50, flush=True)
        print(response.candidates[0].content.parts[0].text, flush=True)
        print("=" * 50, flush=True)

        return response.candidates[0].content.parts[0].text
    except Exception as e:
        logger.error(f"Error: {e}")
        return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ Ä‘Æ°a ra lá»i khuyÃªn lÃºc nÃ y."

# --- 6. ENDPOINTS ---

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "models_loaded": len(MODELS) > 0,
        "gemini_available": GEMINI_CLIENT is not None
    }


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "DiabeTwin AI Backend",
        "version": "2.0",
        "endpoints": {
            "health": "/health",
            "clinical": "/api/predict/clinical",
            "home": "/api/predict/home",
            "docs": "/docs"
        }
    }


@app.post("/api/predict/clinical")
async def predict_clinical(data: ClinicalInput):
    """Clinical prediction endpoint (Doctor mode) - Chá»‰ thay Ä‘á»•i mÃ¡y há»c, giá»¯ nguyÃªn nghiá»‡p vá»¥"""
    try:
        # LOGIC NGHIá»†P Vá»¤: Logger giá»¯ nguyÃªn
        logger.info(f"Clinical prediction request: {data.model_dump()}")
        clean_smoking = data.smoking_history.replace('not current', 'not_current').replace('No Info', 'no_info')
        # THAY Äá»”I CÃCH Xá»¬ LÃ: Sá»­ dá»¥ng Pipeline táº­p trung thay vÃ¬ bÃ³c tÃ¡ch scaler/encoder thá»§ cÃ´ng
        # Äiá»u nÃ y Ä‘áº£m báº£o OutlierClipper (nghiá»‡p vá»¥ xá»­ lÃ½ ngoáº¡i lá»‡ má»›i) Ä‘Æ°á»£c Ã¡p dá»¥ng
        raw_input = pd.DataFrame([{
            'gender': data.gender,
            'age': data.age,
            'hypertension': data.hypertension,
            'heart_disease': data.heart_disease,
            'smoking_history': clean_smoking,
            'bmi': data.bmi,
            'HbA1c_level': data.hba1c,  # Map Ä‘Ãºng tÃªn cá»™t táº­p train
            'blood_glucose_level': data.glucose  # Map Ä‘Ãºng tÃªn cá»™t táº­p train
        }])

        # Cháº¡y qua bá»™ tiá»n xá»­ lÃ½ má»›i (bao gá»“m Clipping + Scaling + Encoding)
        processed_data = MODELS['preprocessor_clinical'].transform(raw_input)

        # Dá»° ÄOÃN: Sá»­ dá»¥ng model AdaBoost má»›i Ä‘Ã£ train
        model = MODELS['model_clinical']
        prob = float(model.predict_proba(processed_data)[0][1])

        # LOGIC SHAP: Giá»¯ nguyÃªn cÃ¡ch tÃ­nh nhÆ°ng cáº­p nháº­t dá»¯ liá»‡u Ä‘áº§u vÃ o
        f = lambda x: model.predict_proba(x)[:, 1]
        # Background Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ sáºµn khi khá»Ÿi Ä‘á»™ng server Ä‘á»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™
        background = MODELS['clinical_background_processed']
        explainer = shap.Explainer(f, background)
        shap_values = explainer(processed_data)

        # NGHIá»†P Vá»¤ CÅ¨: Map tÃªn tiáº¿ng Viá»‡t cho cÃ¡c tÃ­nh nÄƒng
        impacts = []
        # LÆ°u Ã½: VÃ¬ dÃ¹ng OHE nÃªn sá»‘ lÆ°á»£ng feature sau transform sáº½ nhiá»u hÆ¡n 8.
        # ChÃºng ta sáº½ map láº¡i theo logic nghiá»‡p vá»¥ hiá»ƒn thá»‹ cá»§a báº¡n.
        feature_names_out = MODELS['preprocessor_clinical'].get_feature_names_out()

        for i, val in enumerate(shap_values.values[0]):
            impacts.append({
                "feature": feature_names_out[i],
                "impact": round(val * 100, 2)
            })

        # NGHIá»†P Vá»¤ CÅ¨: Gá»i Gemini tÆ° váº¥n
        advice = await get_gemini_advice(prob, impacts, "BÃ¡c sÄ©", data.model_dump())

        # NGHIá»†P Vá»¤ CÅ¨: Loáº¡i bá» Giá»›i tÃ­nh vÃ  HÃºt thuá»‘c khá»i impacts hiá»ƒn thá»‹ frontend
        # (Giá»¯ nguyÃªn logic excluded_features cÅ© cá»§a báº¡n)
        excluded_keywords = ["gender", "smoking_history", "Giá»›i tÃ­nh", "HÃºt thuá»‘c"]
        frontend_impacts = [
            i for i in impacts
            if not any(key in i["feature"] for key in excluded_keywords)
        ]

        # NGHIá»†P Vá»¤ CÅ¨: Cáº¥u trÃºc káº¿t quáº£ tráº£ vá» khÃ´ng Ä‘á»•i
        result = {
            "probability": round(prob * 100, 2),
            # Cáº­p nháº­t Threshold má»›i 0.4945 Ä‘á»ƒ status chÃ­nh xÃ¡c theo model má»›i
            "status": "DÆ¯Æ NG TÃNH" if prob > 0.4945 else "Ã‚M TÃNH",
            "risk_level": "ðŸ”´" if prob > 0.7 else "ðŸŸ¡" if prob > 0.3 else "ðŸŸ¢",
            "impacts": frontend_impacts,
            "ai_advice": advice
        }

        logger.info(f"Clinical prediction successful: {prob * 100:.1f}%")
        return result

    except Exception as e:
        logger.error(f"Clinical prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/predict/home")
async def predict_home(data: HomeInput):
    """Home prediction endpoint (User mode)"""
    try:
        logger.info(f"Home prediction request received")

        df = pd.DataFrame([data.dict()])
        prob = float(MODELS['home_model'].predict_proba(df)[0][1])

        f = lambda x: MODELS['home_model'].predict_proba(x)[:, 1]
        explainer = shap.Explainer(f, MODELS['home_background'])
        shap_values = explainer(df)

        display_names = [
            "Huyáº¿t Ã¡p cao", "Cholesterol cao", "Kiá»ƒm tra Chol", "Chá»‰ sá»‘ BMI",
            "HÃºt thuá»‘c", "Äá»™t quá»µ", "Bá»‡nh tim", "Váº­n Ä‘á»™ng", "TrÃ¡i cÃ¢y",
            "Rau xanh", "RÆ°á»£u bia", "Sá»©c khá»e tá»•ng quÃ¡t", "Sá»©c khá»e tÃ¢m tháº§n",
            "Sá»©c khá»e thá»ƒ cháº¥t", "Äi láº¡i khÃ³", "Giá»›i tÃ­nh", "NhÃ³m tuá»•i"
        ]

        impacts = []
        for i, val in enumerate(shap_values.values[0]):
            impacts.append({"feature": display_names[i], "impact": round(val * 100, 2)})

        advice = await get_gemini_advice(prob, impacts, "NgÆ°á»i dÃ¹ng", data.model_dump())
        excluded_features_home = ["Giá»›i tÃ­nh", "RÆ°á»£u bia", "Sá»©c khá»e tÃ¢m tháº§n", "HÃºt thuá»‘c", "TrÃ¡i cÃ¢y", "Rau xanh"]
        frontend_impacts_home = [i for i in impacts if i["feature"] not in excluded_features_home]
        result = {
            "probability": round(prob * 100, 2),
            "status": "NGUY CÆ  CAO" if prob > 0.5 else "AN TOÃ€N",
            "impacts": frontend_impacts_home,
            "ai_advice": advice
        }

        logger.info(f"Home prediction successful: {prob * 100:.1f}%")
        return result

    except Exception as e:
        logger.error(f"Home prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
