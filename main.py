import joblib
import pandas as pd
import shap
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from contextlib import asynccontextmanager
import logging
import os
from dotenv import load_dotenv
load_dotenv(override=True)
# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- 1. Cáº¤U HÃŒNH AI & BIáº¾N TOÃ€N Cá»¤C ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL_ID = "gemini-2.5-flash"  # Sá»­ dá»¥ng model má»›i nháº¥t
GEMINI_CLIENT = None
print(f"DEBUG: Key Ä‘ang dÃ¹ng lÃ : {GEMINI_API_KEY[:10]}...")

# Khá»Ÿi táº¡o Gemini client vá»›i error handling
def init_gemini_client():
    """Khá»Ÿi táº¡o Gemini client vá»›i xá»­ lÃ½ lá»—i"""
    global GEMINI_CLIENT
    try:
        from google import genai
        GEMINI_CLIENT = genai.Client(api_key=GEMINI_API_KEY)
        logger.info("âœ… Gemini API client initialized successfully")
        return True
    except ImportError:
        logger.error("âŒ google-generativeai package not installed")
        logger.info("Install with: pip install google-generativeai")
        return False
    except Exception as e:
        logger.error(f"âŒ Failed to initialize Gemini client: {e}")
        return False


MODELS = {}
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
def get_path(filename):
    return os.path.join(BASE_DIR, filename)

# --- 2. QUáº¢N LÃ VÃ’NG Äá»œI (LIFESPAN) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # [STARTUP]: Cháº¡y khi server báº¯t Ä‘áº§u
    try:
        MODELS['clinical_model'] = joblib.load(get_path('diabetes_model.pkl'))
        MODELS['clinical_scaler'] = joblib.load(get_path('scaler_diabetes.pkl'))
        MODELS['clinical_encoders'] = joblib.load(get_path('label_encoders.pkl'))
        MODELS['clinical_background'] = joblib.load(get_path('x_train_sample.pkl'))

        # Load ML models cho NgÆ°á»i dÃ¹ng (Home)
        MODELS['home_model'] = joblib.load(get_path('diabetes_model_home.pkl'))
        MODELS['home_background'] = joblib.load(get_path('x_train_sample_home.pkl'))

        logger.info("âœ… [Lifespan] ÄÃ£ táº£i táº¥t cáº£ ML Models thÃ nh cÃ´ng!")

        # Initialize Gemini (non-blocking)
        gemini_ok = init_gemini_client()
        if not gemini_ok:
            logger.warning("âš ï¸ Gemini API khÃ´ng kháº£ dá»¥ng - sáº½ sá»­ dá»¥ng AI advice máº·c Ä‘á»‹nh")

    except Exception as e:
        logger.error(f"âŒ [Lifespan] Lá»—i táº£i model: {e}")
        raise

    yield
    MODELS.clear()
    logger.info("ðŸ§¹ [Lifespan] ÄÃ£ giáº£i phÃ³ng bá»™ nhá»›.")


# --- 3. KHá»žI Táº O APP & CORS ---
app = FastAPI(title="DiabeTwin AI Backend", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://v0-frontend-ai-one.vercel.app",
                  "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- 4. SCHEMA Dá»® LIá»†U ---
class ClinicalInput(BaseModel):
    gender: str
    age: int
    smoking_history: str
    hypertension: int
    heart_disease: int
    bmi: float
    hba1c: float
    glucose: int


class HomeInput(BaseModel):
    HighBP: int
    HighChol: int
    CholCheck: int
    BMI: float
    Smoker: int
    Stroke: int
    HeartDiseaseorAttack: int
    PhysActivity: int
    Fruits: int
    Veggies: int
    HvyAlcoholConsump: int
    GenHlth: int
    MentHlth: int
    PhysHlth: int
    DiffWalk: int
    Sex: int
    Age: int


# --- 5. HELPER FUNCTIONS ---

def generate_fallback_advice(prob: float, impacts: list, role: str, raw_data: dict) -> str:
    """Táº¡o lá»i khuyÃªn máº·c Ä‘á»‹nh khi Gemini khÃ´ng kháº£ dá»¥ng"""

    top_3 = sorted(impacts, key=lambda x: abs(x['impact']), reverse=True)[:3]
    risk_level = "cao" if prob > 0.7 else "trung bÃ¬nh" if prob > 0.4 else "tháº¥p"

    advice = f"""**PhÃ¢n tÃ­ch nguy cÆ¡ tiá»ƒu Ä‘Æ°á»ng**

ðŸŽ¯ **Káº¿t quáº£:** Nguy cÆ¡ á»Ÿ má»©c **{risk_level}** vá»›i xÃ¡c suáº¥t {prob * 100:.1f}%

ðŸ“Š **3 Yáº¿u tá»‘ áº£nh hÆ°á»Ÿng lá»›n nháº¥t:**
"""

    for i, impact in enumerate(top_3, 1):
        direction = "tÄƒng" if impact['impact'] > 0 else "giáº£m"
        advice += f"\n{i}. **{impact['feature']}**: {direction} {abs(impact['impact']):.1f}% nguy cÆ¡"

    advice += "\n\nðŸ’¡ **Khuyáº¿n nghá»‹:**\n"

    # Khuyáº¿n nghá»‹ dá»±a trÃªn risk level
    if prob > 0.7:
        advice += """
1. Kháº©n cáº¥p: Cáº§n gáº·p bÃ¡c sÄ© chuyÃªn khoa tiá»ƒu Ä‘Æ°á»ng trong vÃ²ng 1 tuáº§n
2. Kiá»ƒm tra: XÃ©t nghiá»‡m HbA1c vÃ  Ä‘Æ°á»ng huyáº¿t Ä‘Ã³i ngay
3. Lá»‘i sá»‘ng: Báº¯t Ä‘áº§u cháº¿ Ä‘á»™ Äƒn kiÃªng Ã­t Ä‘Æ°á»ng, tÄƒng váº­n Ä‘á»™ng ngay láº­p tá»©c
"""
    elif prob > 0.4:
        advice += """
1. Theo dÃµi: Äáº·t lá»‹ch khÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³ 3-6 thÃ¡ng/láº§n
2. PhÃ²ng ngá»«a: Äiá»u chá»‰nh cháº¿ Ä‘á»™ Äƒn, tÄƒng váº­n Ä‘á»™ng 30 phÃºt/ngÃ y
3. Kiá»ƒm tra: Theo dÃµi cÃ¡c chá»‰ sá»‘ sá»©c khá»e táº¡i nhÃ 
"""
    else:
        advice += """
1. Duy trÃ¬: Tiáº¿p tá»¥c lá»‘i sá»‘ng lÃ nh máº¡nh hiá»‡n táº¡i
2. Kiá»ƒm tra: KhÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³ hÃ ng nÄƒm
3. PhÃ²ng ngá»«a: Giá»¯ cÃ¢n náº·ng á»•n Ä‘á»‹nh, váº­n Ä‘á»™ng Ä‘á»u Ä‘áº·n
"""

    advice += "\n\nâš ï¸ *LÆ°u Ã½: Káº¿t quáº£ chá»‰ mang tÃ­nh tham kháº£o. Vui lÃ²ng tham kháº£o Ã½ kiáº¿n bÃ¡c sÄ© chuyÃªn khoa.*"

    return advice


async def get_gemini_advice(prob: float, impacts: list, role: str, raw_data: dict) -> str:
    # 1. Táº¡o bá»‘i cáº£nh dá»±a trÃªn Role
    if role == "BÃ¡c sÄ©":
        context = """Báº¡n lÃ  má»™t chuyÃªn gia ná»™i tiáº¿t há»— trá»£ bÃ¡c sÄ©. 
            HÃ£y phÃ¢n tÃ­ch dá»¯ liá»‡u lÃ¢m sÃ ng dÆ°á»›i gÃ³c Ä‘á»™ chuyÃªn mÃ´n y khoa."""

        requirement = """
            HÃ£y tráº£ lá»i theo CHÃNH XÃC format sau (má»—i má»¥c trÃªn 1 dÃ²ng):

    **ÄÃ¡nh giÃ¡ nguy cÆ¡**
    - [Nháº­n Ä‘á»‹nh vá» má»©c Ä‘á»™ nguy cÆ¡]
    - [Giáº£i thÃ­ch vá» cÃ¡c chá»‰ sá»‘ quan trá»ng]

    **Khuyáº¿n nghá»‹ lÃ¢m sÃ ng**
    - [XÃ©t nghiá»‡m cáº§n lÃ m thÃªm]
    - [HÆ°á»›ng Ä‘iá»u trá»‹ Ä‘á» xuáº¥t]
    - [Theo dÃµi cáº§n thiáº¿t]

    **LÆ°u Ã½ Ä‘áº·c biá»‡t**
    - [CÃ¡c chá»‰ sá»‘ cáº§n chÃº Ã½ kháº©n cáº¥p náº¿u cÃ³]
            """
    else:
        context = """Báº¡n lÃ  BÃ¡c sÄ© gia Ä‘Ã¬nh há»— trá»£ ngÆ°á»i dÃ¹ng táº¡i nhÃ . 
HÃ£y giáº£i thÃ­ch káº¿t quáº£ dá»± Ä‘oÃ¡n tiá»ƒu Ä‘Æ°á»ng má»™t cÃ¡ch dá»… hiá»ƒu, gáº§n gÅ©i vÃ  Ä‘áº§y Ä‘á»§ Ä‘á»‹nh lÆ°á»£ng."""

        requirement = f"""
        HÃ£y tráº£ lá»i theo CHÃNH XÃC format sau (má»—i má»¥c trÃªn 1 dÃ²ng):

**ÄÃ¡nh giÃ¡ sá»©c khá»e**
- [Nháº­n Ä‘á»‹nh nguy cÆ¡ dá»±a trÃªn xÃ¡c suáº¥t]
- [Giáº£i thÃ­ch Ã½ nghÄ©a cÃ¡c chá»‰ sá»‘ áº£nh hÆ°á»Ÿng chÃ­nh Ä‘áº¿n dá»± Ä‘oÃ¡n]

**Lá»i khuyÃªn hÃ nh Ä‘á»™ng (Cá»¥ thá»ƒ sá»‘ liá»‡u)**
- [Cháº¿ Ä‘á»™ Äƒn: Ä‚n gÃ¬, bá» gÃ¬, Ä‘á»‹nh lÆ°á»£ng gram/bá»¯a tháº¿ nÃ o?]
- [Váº­n Ä‘á»™ng: Táº­p mÃ´n gÃ¬, bao nhiÃªu phÃºt/ngÃ y, bao nhiÃªu ngÃ y/tuáº§n?]
- [Má»¥c tiÃªu: Cáº§n giáº£m bao nhiÃªu kg, Ä‘Æ°a chá»‰ sá»‘ vá» má»©c bao nhiÃªu?]

**LÆ°u Ã½ quan trá»ng**
- [Dáº¥u hiá»‡u cáº§n Ä‘i khÃ¡m ngay hoáº·c lá»i nháº¯c tÃ¡i khÃ¡m]

# QUY Táº®C Cá» Äá»ŠNH
1. KHÃ”NG chÃ o há»i xÃ£ giao. Báº¯t Ä‘áº§u ngay báº±ng pháº§n ÄÃ¡nh giÃ¡.
2. Má»i lá»i khuyÃªn PHáº¢I gáº¯n liá»n vá»›i con sá»‘ cá»¥ thá»ƒ cá»§a bá»‡nh nhÃ¢n (BMI, Glucose...).
3. Ãt nháº¥t 9-10 gá»£i Ã½ chi tiáº¿t chia Ä‘á»u cho cÃ¡c má»¥c.
                """

    prompt = f"""
    {context}

    **Dá»¯ liá»‡u bá»‡nh nhÃ¢n:**
    - Nguy cÆ¡ tiá»ƒu Ä‘Æ°á»ng: {prob * 100:.1f}%
    - CÃ¡c yáº¿u tá»‘ áº£nh hÆ°á»Ÿng chÃ­nh: {impacts}
    - ThÃ´ng tin chi tiáº¿t: {raw_data}

    **YÃŠU Cáº¦U QUAN TRá»ŒNG:**
    {requirement}

    CHÃš Ã: 
    - Má»—i gá»£i Ã½ pháº£i lÃ  1 cÃ¢u hoÃ n chá»‰nh, cá»¥ thá»ƒ, cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c
    - Sá»­ dá»¥ng dáº¥u - á»Ÿ Ä‘áº§u má»—i dÃ²ng
    - KHÃ”NG thÃªm sá»‘ thá»© tá»±, KHÃ”NG thÃªm header phá»©c táº¡p
    - Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
    """

    try:

        response = GEMINI_CLIENT.models.generate_content(
            model=GEMINI_MODEL_ID,
            contents=prompt
        )
        print("=" * 50, flush=True)
        print("GEMINI RESPONSE:", flush=True)
        print("=" * 50, flush=True)
        print(response.candidates[0].content.parts[0].text, flush=True)
        print("=" * 50, flush=True)

        return response.candidates[0].content.parts[0].text
    except Exception as e:
        logger.error(f"Error: {e}")
        return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ Ä‘Æ°a ra lá»i khuyÃªn lÃºc nÃ y."

# --- 6. ENDPOINTS ---

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "models_loaded": len(MODELS) > 0,
        "gemini_available": GEMINI_CLIENT is not None
    }


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "DiabeTwin AI Backend",
        "version": "2.0",
        "endpoints": {
            "health": "/health",
            "clinical": "/api/predict/clinical",
            "home": "/api/predict/home",
            "docs": "/docs"
        }
    }


@app.post("/api/predict/clinical")
async def predict_clinical(data: ClinicalInput):
    """Clinical prediction endpoint (Doctor mode)"""
    try:
        logger.info(f"Clinical prediction request: {data.model_dump()}")

        encoders = MODELS['clinical_encoders']
        scaler = MODELS['clinical_scaler']

        input_list = [
            encoders['gender'].transform([data.gender])[0],
            data.age, data.hypertension, data.heart_disease,
            encoders['smoking_history'].transform([data.smoking_history])[0],
            data.bmi, data.hba1c, data.glucose
        ]

        df = pd.DataFrame([input_list], columns=MODELS['clinical_background'].columns)
        scaled_df = pd.DataFrame(scaler.transform(df), columns=df.columns)

        prob = float(MODELS['clinical_model'].predict_proba(scaled_df)[0][1])

        # SHAP Calculation
        f = lambda x: MODELS['clinical_model'].predict_proba(x)[:, 1]
        background = scaler.transform(MODELS['clinical_background'].sample(100))
        explainer = shap.Explainer(f, background)
        shap_values = explainer(scaled_df)

        impacts = []
        features = ["Giá»›i tÃ­nh", "Tuá»•i", "Huyáº¿t Ã¡p", "Bá»‡nh tim", "HÃºt thuá»‘c", "BMI", "HbA1c", "ÄÆ°á»ng huyáº¿t"]
        for i, val in enumerate(shap_values.values[0]):
            impacts.append({"feature": features[i], "impact": round(val * 100, 2)})

        advice = await get_gemini_advice(prob, impacts, "BÃ¡c sÄ©", data.model_dump())

        excluded_features = ["Giá»›i tÃ­nh", "HÃºt thuá»‘c"]
        frontend_impacts = [i for i in impacts if i["feature"] not in excluded_features]

        result = {
            "probability": round(prob * 100, 2),
            "status": "DÆ¯Æ NG TÃNH" if prob > 0.5 else "Ã‚M TÃNH",
            "risk_level": "ðŸ”´" if prob > 0.7 else "ðŸŸ¡" if prob > 0.3 else "ðŸŸ¢",
            "impacts": frontend_impacts,
            "ai_advice": advice
        }

        logger.info(f"Clinical prediction successful: {prob * 100:.1f}%")
        return result

    except Exception as e:
        logger.error(f"Clinical prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/predict/home")
async def predict_home(data: HomeInput):
    """Home prediction endpoint (User mode)"""
    try:
        logger.info(f"Home prediction request received")

        df = pd.DataFrame([data.dict()])
        prob = float(MODELS['home_model'].predict_proba(df)[0][1])

        f = lambda x: MODELS['home_model'].predict_proba(x)[:, 1]
        explainer = shap.Explainer(f, MODELS['home_background'])
        shap_values = explainer(df)

        display_names = [
            "Huyáº¿t Ã¡p cao", "Cholesterol cao", "Kiá»ƒm tra Chol", "Chá»‰ sá»‘ BMI",
            "HÃºt thuá»‘c", "Äá»™t quá»µ", "Bá»‡nh tim", "Váº­n Ä‘á»™ng", "TrÃ¡i cÃ¢y",
            "Rau xanh", "RÆ°á»£u bia", "Sá»©c khá»e tá»•ng quÃ¡t", "Sá»©c khá»e tÃ¢m tháº§n",
            "Sá»©c khá»e thá»ƒ cháº¥t", "Äi láº¡i khÃ³", "Giá»›i tÃ­nh", "NhÃ³m tuá»•i"
        ]

        impacts = []
        for i, val in enumerate(shap_values.values[0]):
            impacts.append({"feature": display_names[i], "impact": round(val * 100, 2)})

        advice = await get_gemini_advice(prob, impacts, "NgÆ°á»i dÃ¹ng", data.model_dump())
        excluded_features_home = ["Giá»›i tÃ­nh", "RÆ°á»£u bia", "Sá»©c khá»e tÃ¢m tháº§n", "HÃºt thuá»‘c", "TrÃ¡i cÃ¢y", "Rau xanh"]
        frontend_impacts_home = [i for i in impacts if i["feature"] not in excluded_features_home]
        result = {
            "probability": round(prob * 100, 2),
            "status": "NGUY CÆ  CAO" if prob > 0.5 else "AN TOÃ€N",
            "impacts": frontend_impacts_home,
            "ai_advice": advice
        }

        logger.info(f"Home prediction successful: {prob * 100:.1f}%")
        return result

    except Exception as e:
        logger.error(f"Home prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
